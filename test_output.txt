============================= test session starts =============================
platform win32 -- Python 3.10.11, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\Jonatas Lampa\AppData\Local\Programs\Python\Python310\python.exe
cachedir: .pytest_cache
metadata: {'Python': '3.10.11', 'Platform': 'Windows-10-10.0.26200-SP0', 'Packages': {'pytest': '9.0.2', 'pluggy': '1.6.0'}, 'Plugins': {'anyio': '4.12.1', 'Faker': '40.1.2', 'cov': '7.0.0', 'html': '4.2.0', 'metadata': '3.1.1', 'timeout': '2.4.0'}}
rootdir: C:\plugin_autocad
plugins: anyio-4.12.1, Faker-40.1.2, cov-7.0.0, html-4.2.0, metadata-3.1.1, timeout-2.4.0
collecting ... collected 20 items

tests/test_api_contracts.py::test_health_contract ERROR                  [  5%]
tests/test_api_contracts.py::test_auth_check_contract_valid ERROR        [ 10%]
tests/test_api_contracts.py::test_auth_check_contract_invalid ERROR      [ 15%]
tests/test_api_contracts.py::test_health_detailed_contract ERROR         [ 20%]
tests/test_api_contracts.py::test_create_job_contract ERROR              [ 25%]
tests/test_api_contracts.py::test_get_job_contract ERROR                 [ 30%]
tests/test_api_contracts.py::test_get_job_not_found_contract ERROR       [ 35%]
tests/test_api_contracts.py::test_cancel_job_contract ERROR              [ 40%]
tests/test_api_contracts.py::test_elevation_query_contract ERROR         [ 45%]
tests/test_api_contracts.py::test_elevation_profile_contract ERROR       [ 50%]
tests/test_api_contracts.py::test_ai_chat_contract ERROR                 [ 55%]
tests/test_api_contracts.py::test_webhook_register_contract ERROR        [ 60%]
tests/test_api_contracts.py::test_event_emit_contract ERROR              [ 65%]
tests/test_api_contracts.py::test_update_project_not_found_contract ERROR [ 70%]
tests/test_api_contracts.py::test_error_response_format_unauthorized ERROR [ 75%]
tests/test_api_contracts.py::test_error_response_format_not_found ERROR  [ 80%]
tests/test_api_contracts.py::test_auth_header_requirement ERROR          [ 85%]
tests/test_api_contracts.py::test_job_creation_idempotency ERROR         [ 90%]
tests/test_api_contracts.py::test_backward_compatibility_v070_health ERROR [ 95%]
tests/test_api_contracts.py::test_backward_compatibility_v070_job_create ERROR [100%]

=================================== ERRORS ====================================
___________________ ERROR at setup of test_health_contract ____________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
---------------------------- Captured stdout setup ----------------------------
[webhooks] Service initialized. Static listeners: 0
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:23.815919Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
______________ ERROR at setup of test_auth_check_contract_valid _______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.055621Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_____________ ERROR at setup of test_auth_check_contract_invalid ______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.079974Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_______________ ERROR at setup of test_health_detailed_contract _______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.104089Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_________________ ERROR at setup of test_create_job_contract __________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.127708Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
___________________ ERROR at setup of test_get_job_contract ___________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.156220Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
______________ ERROR at setup of test_get_job_not_found_contract ______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.183724Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_________________ ERROR at setup of test_cancel_job_contract __________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.210300Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_______________ ERROR at setup of test_elevation_query_contract _______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.234796Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
______________ ERROR at setup of test_elevation_profile_contract ______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.282241Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
___________________ ERROR at setup of test_ai_chat_contract ___________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.304405Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
______________ ERROR at setup of test_webhook_register_contract _______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.330254Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_________________ ERROR at setup of test_event_emit_contract __________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.355385Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
__________ ERROR at setup of test_update_project_not_found_contract ___________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.382593Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
__________ ERROR at setup of test_error_response_format_unauthorized __________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.408393Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
___________ ERROR at setup of test_error_response_format_not_found ____________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.432719Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_______________ ERROR at setup of test_auth_header_requirement ________________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.481809Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
_______________ ERROR at setup of test_job_creation_idempotency _______________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.508500Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
__________ ERROR at setup of test_backward_compatibility_v070_health __________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.534060Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
________ ERROR at setup of test_backward_compatibility_v070_job_create ________

    @pytest.fixture
    def client():
        """Create test client with authentication."""
        import os
        os.environ["SISRUA_AUTH_TOKEN"] = TEST_TOKEN
    
        # Import after setting env var
>       from backend.api import app

tests\test_api_contracts.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    import os
    import sys
    import time
    import threading
    from pathlib import Path
    from typing import Dict, Any, List, Optional
    
    # Configure Matplotlib backend to 'Agg' BEFORE any other matplotlib imports
    # this ensures thread-safety and avoids GUI-related memory leaks in headless environments.
    try:
        import matplotlib
        matplotlib.use('Agg')
    except ImportError:
        pass
    
    from fastapi import FastAPI, HTTPException, Header, Request
    from fastapi.middleware.cors import CORSMiddleware
    from starlette.responses import Response
    
    
    # --- Sentry SDK for Error Monitoring ---
    import sentry_sdk
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.starlette import StarletteIntegration
    
    # --- Metrics / Logging v2 ---
    import uuid
    import structlog
    from backend.core.logger import configure_logging, get_logger, set_trace_id
    
    configure_logging()
    logger = get_logger(__name__)
    
    
    
    
    app = FastAPI(
        title="sisRUA API",
        version="0.8.0",
        description="""
    **sisRUA** - Generative Urban Design System for AutoCAD.
    
    This API powers the AutoCAD plugin for:
    - Fetching and projecting **OpenStreetMap** data
    - Processing **GeoJSON** files for CAD import
    - Providing **elevation data** from SRTM sources
    - Managing **asynchronous jobs** for long-running operations
    
    ## Authentication
    Protected endpoints require the `X-SisRua-Token` header.
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        openapi_tags=[
            {"name": "Health", "description": "Health check endpoints"},
            {"name": "Jobs", "description": "Asynchronous job management"},
            {"name": "Prepare", "description": "Data preparation (OSM/GeoJSON)"},
            {"name": "Tools", "description": "Utility tools (elevation, etc.)"},
            {"name": "Webhooks", "description": "Dynamic webhook registration"},
            {"name": "Projects", "description": "Project metadata management"},
            {"name": "AI", "description": "AI-powered chat assistance"},
            {"name": "Audit", "description": "Cryptographic audit logging"},
        ]
    )
    
    try:
        from prometheus_fastapi_instrumentator import Instrumentator
        # Initialize Prometheus Metrics
        Instrumentator().instrument(app).expose(app)
    except ImportError:
        logger.warning("prometheus_metrics_unavailable", error="Module not found")
    
    # Middleware for Audit Logging (Trace ID)
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        trace_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
        set_trace_id(trace_id)
    
        structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
    
        response.headers["X-Request-ID"] = trace_id
    
        logger.info("request_processed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                    duration=process_time)
    
        return response
    
    SENTRY_DSN = os.environ.get("SENTRY_DSN")
    if SENTRY_DSN:
        sentry_sdk.init(
            dsn=SENTRY_DSN,
            integrations=[
                StarletteIntegration(transaction_style="endpoint"),
                FastApiIntegration(transaction_style="endpoint"),
            ],
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,
            environment=os.environ.get("SENTRY_ENVIRONMENT", "development"),
            release=f"sisrua-backend@0.7.0",
            send_default_pii=False,  # Do not send personally identifiable information
        )
    
    
    # --- New Imports from SoC ---
    from backend.models import (
        PrepareOsmRequest, PrepareGeoJsonRequest, PrepareJobRequest,
        PrepareResponse, JobStatusResponse, ElevationQueryRequest,
        ElevationProfileRequest, CadFeature, HealthResponse,
        ElevationPointResponse, ElevationProfileResponse, WebhookRegistrationRequest,
        InternalEvent
    )
    from backend.services.jobs import (
        job_store, init_job, update_job, check_cancellation,
        get_job, cancel_job
    )
    from backend.services.webhooks import webhook_service
    from backend.services.osm import prepare_osm_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.services.geojson import prepare_geojson_compute
    from backend.core.utils import sanitize_jsonable
    from backend.core.rate_limit import RateLimiter
    from fastapi import Depends
    
    AUTH_TOKEN = os.environ.get("SISRUA_AUTH_TOKEN") or ""
    AUTH_HEADER_NAME = "X-SisRua-Token"
    
    def _require_token(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """
        Protege endpoints sens�veis contra chamadas externas na m�quina do usu�rio.
        Pol�tica: FAIL CLOSED. Se o servidor n�o tiver token configurado, ningu�m entra.
        """
        if not AUTH_TOKEN:
            # Server misconfiguration or strict lockdown
            raise HTTPException(status_code=500, detail="Server Authentication Not Configured")
    
        if not x_sisrua_token or x_sisrua_token != AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Unauthorized")
    
    # --- CORS Middleware ---
    # Allows requests from the WebView2 control (localhost origins)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "http://localhost:8000",
            "http://127.0.0.1:8000",
            "http://localhost:5173",  # Vite dev server
            "http://127.0.0.1:5173",
            "http://localhost:4173",  # Vite preview
        ],
        allow_credentials=True,
        allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["X-SisRua-Token"],
    )
    
    # --- Security Headers Middleware ---
    @app.middleware("http")
    async def add_security_headers(request: Request, call_next):
        """Adds security headers to all responses efficiently."""
        response = await call_next(request)
    
        # Prevent XSS attacks
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-XSS-Protection"] = "1; mode=block"
    
        # Prevent clickjacking
        response.headers["X-Frame-Options"] = "DENY"
    
        # Content Security Policy (relaxed for WebView2 compatibility)
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
    
        # Referrer Policy
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
        return response
    
    # --- Warm-up logic (Internal performance) ---
    @app.on_event("startup")
    async def startup_event():
        """Warms up the environment to reduce latency on first user request."""
        # Start background cleanup thread for jobs
        def run_cleanup():
            from backend.services.jobs import cleanup_expired_jobs
            while True:
                try:
                    count = cleanup_expired_jobs(max_age_seconds=3600)
                    if count > 0:
                        print(f"[cleanup] Removed {count} expired jobs.")
                except Exception as e:
                    print(f"[cleanup] Error: {e}")
                time.sleep(600) # Run every 10 minutes
    
        cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
        cleanup_thread.start()
    
        # Pre-calculate or pre-import anything that doesn't depend on heavy GIS libs
        # (Heavy GIS libs are deferred to usage time now)
        print("[startup] sisRUA API ready.")
    
    
    
    @app.get("/api/v1/auth/check", tags=["Health"], response_model=HealthResponse)
    async def auth_check(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Check if the provided authentication token is valid."""
        _require_token(x_sisrua_token)
        return HealthResponse(status="ok")
    
    @app.get("/api/v1/health", tags=["Health"], response_model=HealthResponse)
    async def health():
        """Simple health check to verify the API server is up and running."""
        return HealthResponse(status="ok")
    
    from backend.models import DeepHealthResponse
    from backend.services.health import health_service
    
    @app.get("/api/v1/health/detailed", tags=["Health"], response_model=DeepHealthResponse)
    async def health_detailed(x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)):
        """Deep health check verifying DB, Cache, and Configuration."""
        _require_token(x_sisrua_token) # Deep check is protected
        return health_service.check_health()
    
    from backend.services.projects import ProjectService, ConflictError, NotFoundError
    from backend.models import ProjectUpdateRequest
    
    @app.put("/api/v1/projects/{project_id}", tags=["Projects"])
    async def update_project(
        project_id: str,
        req: ProjectUpdateRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Update project metadata safely using Optimistic Locking.
        Requires 'version' in body matching the database version.
        """
        _require_token(x_sisrua_token)
        try:
            updated = project_service.update_project(
                project_id=project_id,
                updates=req.model_dump(exclude={"version"}, exclude_unset=True),
                expected_version=req.version
            )
            return updated
        except ConflictError as e:
            raise HTTPException(status_code=409, detail=str(e))
        except NotFoundError as e:
            raise HTTPException(status_code=404, detail=str(e))
    
    from backend.services.cache import cache_service
    from backend.core.bus import InMemoryEventBus
    
    # --- Composition Root ---
    event_bus = InMemoryEventBus(cache=cache_service)
    project_service = ProjectService(event_bus=event_bus)
    from backend.services.executor import JobExecutor
    job_executor = JobExecutor(cache_service=cache_service)
    
    # Wiring: WebhookService listens to events
    def webhook_adapter(payload: Dict[str, Any]):
        pass
    
    # Direct subscriptions for known job events
    event_bus.subscribe("job_started", lambda p: webhook_service.broadcast("job_started", p))
    event_bus.subscribe("job_completed", lambda p: webhook_service.broadcast("job_completed", p))
    event_bus.subscribe("job_failed", lambda p: webhook_service.broadcast("job_failed", p))
    event_bus.subscribe("project_saved", lambda p: webhook_service.broadcast("project_saved", p))
    event_bus.subscribe("project_updated", lambda p: webhook_service.broadcast("project_updated", p))
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Graceful shutdown handler."""
        print("[shutdown] Stopping background services...")
    
        # Signal shutdown
        from backend.core.lifecycle import SHUTDOWN_EVENT, job_registry
        SHUTDOWN_EVENT.set()
    
        # Wait for active jobs
        job_registry.wait_for_completion(timeout=10.0)
        print("[shutdown] Bye.")
    
    def _run_prepare_job_sync(job_id: str, payload: PrepareJobRequest, trace_id: str) -> None:
        try:
            # Restore Trace Context in the new thread
            set_trace_id(trace_id)
            structlog.contextvars.bind_contextvars(trace_id=trace_id)
    
            job_executor.execute_prepare_job(job_id, payload, event_bus)
        finally:
            from backend.core.lifecycle import job_registry
            job_registry.remove(threading.current_thread())
    
    
    @app.post("/api/v1/jobs/prepare", tags=["Jobs"], response_model=JobStatusResponse)
    async def create_prepare_job(
        payload: PrepareJobRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME),
        _ = Depends(RateLimiter(calls=5, period=60)) # 5 jobs per minute per IP
    ):
        """Start an asynchronous data preparation job (OSM or GeoJSON). returns the initial job status."""
        _require_token(x_sisrua_token)
    
        try:
            import hashlib
            import json
            payload_dict = payload.model_dump()
            payload_json = json.dumps(payload_dict, sort_keys=True)
            idempotency_key = hashlib.sha256(payload_json.encode()).hexdigest()
    
            from backend.core.logger import get_trace_id
            current_trace_id = get_trace_id()
            from backend.core.lifecycle import job_registry
    
            job_id, is_new = init_job(payload.kind, idempotency_key=idempotency_key)
    
            if is_new:
                t = threading.Thread(target=_run_prepare_job_sync, args=(job_id, payload, current_trace_id), daemon=True)
                job_registry.add(t)
                t.start()
            else:
                logger.info("job_creation_skipped_dedup", job_id=job_id)
    
            return get_job(job_id)
        except Exception as e:
            logger.error("create_job_failed", error=str(e), traceback=True)
            raise e
    
    @app.get("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=JobStatusResponse)
    async def get_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve the current status, progress, and result (if completed) of a job."""
        _require_token(x_sisrua_token)
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        return job
    
    @app.delete("/api/v1/jobs/{job_id}", tags=["Jobs"], response_model=HealthResponse)
    async def cancel_job_endpoint(
        job_id: str,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Request cancellation of a running job."""
        _require_token(x_sisrua_token)
    
        # Use service method
        cancelled = cancel_job(job_id)
        if not cancelled:
            # Check if it was because it wasn't found or because it was already done
            job = get_job(job_id)
            if not job:
                raise HTTPException(status_code=404, detail="Job not found")
            # if found but return false, it means it was already done, which counts as success/ignored?
            # api spec says if already finished do nothing and return ok.
    
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/tools/elevation/query", tags=["Tools"], response_model=ElevationPointResponse)
    async def query_elevation(
        req: ElevationQueryRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Query numeric elevation (Z) at a single lat/lon point."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            z = svc.get_elevation_at_point(req.latitude, req.longitude)
            return ElevationPointResponse(latitude=req.latitude, longitude=req.longitude, elevation=z)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/api/v1/tools/elevation/profile", tags=["Tools"], response_model=ElevationProfileResponse)
    async def query_profile(
        req: ElevationProfileRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Retrieve an elevation profile (list of Z values) along a given path."""
        _require_token(x_sisrua_token)
        try:
            from backend.services.elevation import ElevationService
            svc = ElevationService(cache=cache_service)
            # Convert list of lists to list of tuples for the service
            coords = [(p[0], p[1]) for p in req.path]
            elevations = svc.get_elevation_profile(coords)
            return ElevationProfileResponse(elevations=elevations)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    from backend.services.ai import AiService
    from pydantic import BaseModel
    
    class ChatRequest(BaseModel):
        message: str
        context: Optional[Dict[str, Any]] = None
        job_id: Optional[str] = None
    
    class ChatResponse(BaseModel):
        response: str
    
    ai_service = AiService()
    
    @app.post("/api/v1/ai/chat", tags=["AI"], response_model=ChatResponse)
    async def chat_with_ai(
        req: ChatRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Interact with sisRUA AI based on Groq."""
        _require_token(x_sisrua_token)
        try:
            reply = ai_service.generate_response(req.message, req.context, req.job_id)
            return ChatResponse(response=reply)
        except Exception as e:
            # Graceful degradation
            return ChatResponse(response="AI unavailable.")
    
    
    @app.post("/api/v1/prepare/osm", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_osm(
        req: PrepareOsmRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous OSM data preparation.
        Downloads OSM data in lat/lon (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        from backend.services.elevation import ElevationService
        elev_svc = ElevationService(cache=cache_service)
        return prepare_osm_compute(
            req.latitude,
            req.longitude,
            req.radius,
            cache_service=cache_service,
            elevation_service=elev_svc
        )
    
    @app.post("/api/v1/prepare/geojson", tags=["Prepare"], response_model=PrepareResponse)
    async def prepare_geojson(
        req: PrepareGeoJsonRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Synchronous GeoJSON data preparation.
        Accepts GeoJSON (EPSG:4326), projects to SIRGAS 2000 UTM,
        and returns a list of CAD-ready features.
        """
        _require_token(x_sisrua_token)
        return prepare_geojson_compute(req.geojson)
    
    @app.post("/api/v1/webhooks/register", tags=["Webhooks"], response_model=HealthResponse)
    async def register_webhook(
        req: WebhookRegistrationRequest,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """Register a new URL to receive system events via webhook."""
        _require_token(x_sisrua_token)
        webhook_service.register_url(req.url)
        return HealthResponse(status="ok")
    
    @app.post("/api/v1/events/emit", tags=["Webhooks"], response_model=HealthResponse)
    async def emit_event(
        req: InternalEvent,
        x_sisrua_token: str | None = Header(default=None, alias=AUTH_HEADER_NAME)
    ):
        """
        Internal endpoint for the AutoCAD plugin to emit events for webhook broadcasting.
        e.g. project_saved, project_loaded.
        """
        _require_token(x_sisrua_token)
        webhook_service.broadcast(req.event_type, req.payload)
        return HealthResponse(status="ok")
    
    # --- Audit Log Routes ---
>   from backend.api.audit_routes import audit_bp
E   ModuleNotFoundError: No module named 'backend.api.audit_routes'; 'backend.api' is not a package

src\backend\backend\api.py:487: ModuleNotFoundError
----------------------------- Captured log setup ------------------------------
WARNING  backend.api:api.py:74 {"error": "Module not found", "event": "prometheus_metrics_unavailable", "level": "warning", "timestamp": "2026-02-02T00:18:24.559814Z"}
WARNING  backend.services.ai:ai.py:15 GROQ_API_KEY not set. AI features will be disabled/mocked.
============================== warnings summary ===============================
tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:9: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    status: str = Field(..., description="Operational status of the API", example="ok")

tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:26: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    latitude: float = Field(..., description="Target latitude (EPSG:4326)", example=-21.7634)

tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:27: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    longitude: float = Field(..., description="Target longitude (EPSG:4326)", example=-41.3235)

tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:28: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    radius: float = Field(..., description="Search radius in meters", example=500.0)

tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:63: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    crs_out: Optional[str] = Field(None, description="Projected Coordinate Reference System", example="EPSG:31983")

tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:94: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    url: str = Field(..., description="Target URL to receive webhook events", example="https://example.com/webhook")

tests/test_api_contracts.py::test_health_contract
  c:\plugin_autocad\src\backend\backend\models.py:98: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    event_type: str = Field(..., description="Type of the internal event", example="project_saved")

tests/test_api_contracts.py: 20 warnings
  c:\plugin_autocad\src\backend\backend\api.py:188: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")

tests/test_api_contracts.py: 40 warnings
  C:\Users\Jonatas Lampa\AppData\Local\Programs\Python\Python310\lib\site-packages\fastapi\applications.py:4576: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    return self.router.on_event(event_type)

tests/test_api_contracts.py: 20 warnings
  c:\plugin_autocad\src\backend\backend\api.py:278: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("shutdown")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR tests/test_api_contracts.py::test_health_contract - ModuleNotFoundError...
ERROR tests/test_api_contracts.py::test_auth_check_contract_valid - ModuleNot...
ERROR tests/test_api_contracts.py::test_auth_check_contract_invalid - ModuleN...
ERROR tests/test_api_contracts.py::test_health_detailed_contract - ModuleNotF...
ERROR tests/test_api_contracts.py::test_create_job_contract - ModuleNotFoundE...
ERROR tests/test_api_contracts.py::test_get_job_contract - ModuleNotFoundErro...
ERROR tests/test_api_contracts.py::test_get_job_not_found_contract - ModuleNo...
ERROR tests/test_api_contracts.py::test_cancel_job_contract - ModuleNotFoundE...
ERROR tests/test_api_contracts.py::test_elevation_query_contract - ModuleNotF...
ERROR tests/test_api_contracts.py::test_elevation_profile_contract - ModuleNo...
ERROR tests/test_api_contracts.py::test_ai_chat_contract - ModuleNotFoundErro...
ERROR tests/test_api_contracts.py::test_webhook_register_contract - ModuleNot...
ERROR tests/test_api_contracts.py::test_event_emit_contract - ModuleNotFoundE...
ERROR tests/test_api_contracts.py::test_update_project_not_found_contract - M...
ERROR tests/test_api_contracts.py::test_error_response_format_unauthorized - ...
ERROR tests/test_api_contracts.py::test_error_response_format_not_found - Mod...
ERROR tests/test_api_contracts.py::test_auth_header_requirement - ModuleNotFo...
ERROR tests/test_api_contracts.py::test_job_creation_idempotency - ModuleNotF...
ERROR tests/test_api_contracts.py::test_backward_compatibility_v070_health - ...
ERROR tests/test_api_contracts.py::test_backward_compatibility_v070_job_create
======================= 87 warnings, 20 errors in 1.31s =======================
